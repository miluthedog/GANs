{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "data: MNIST only 6\n",
    "  input: 28x28 pixels of value (0->1)\n",
    "  output: encode and decode MNIST\n",
    "\n",
    "framework: tf.keras\n",
    "model: VAE\n",
    "  layers:\n",
    "    encoder\n",
    "    decoder\n",
    "  params:\n",
    "  hyperparams:\n",
    "  algorithm: VAE-2 MLP\n",
    "\n",
    "result: works, smoother and easier than vanilla GAN\n",
    "  test: \n",
    "  5 iterations for batch = 32 in dataset (~900) -> convergence already. Gen image look really good\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open(\"10kimages.idx3-ubyte\", \"rb\") as file:\n",
    "    magic, num, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "    data = np.frombuffer(file.read(), dtype=np.uint8)\n",
    "    features = data.reshape(num, rows * cols).astype(np.float32) / 255.0\n",
    "\n",
    "with open(\"10klabels.idx1-ubyte\", \"rb\") as file:\n",
    "    magic, num = struct.unpack(\">II\", file.read(8))\n",
    "    labels = np.frombuffer(file.read(), dtype=np.uint8)\n",
    "\n",
    "indices = np.where(labels == 6)[0]\n",
    "images = features[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 28*28\n",
    "latent_size = 28 # mu and log sigma^2\n",
    "batch_size = 32\n",
    "\n",
    "a = images[4].reshape(28,28)\n",
    "plt.imshow(a, cmap = 'gray')\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(images).shuffle(images.shape[0]).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def models():\n",
    "    global encoder, enopti, decoder, deopti\n",
    "\n",
    "    encoder = tf.keras.models.Sequential([\n",
    "        tf.keras.Input(shape=(image_size,)),\n",
    "        tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(latent_size * 2)])\n",
    "    enopti = tf.keras.optimizers.Adam()\n",
    "\n",
    "    decoder = tf.keras.models.Sequential([\n",
    "        tf.keras.Input(shape=(latent_size,)),\n",
    "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(image_size, activation=\"sigmoid\")])\n",
    "    deopti = tf.keras.optimizers.Adam()\n",
    "\n",
    "models()\n",
    "\n",
    "def parameterization_trick(mu, log_sigma2):\n",
    "    eps = tf.random.normal(shape=tf.shape(mu))\n",
    "    sigma = tf.exp(0.5 * log_sigma2)\n",
    "    return mu + (sigma * eps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameterization_trick(mu, log_sigma2):\n",
    "    eps = tf.random.normal(shape=tf.shape(mu))\n",
    "    sigma = tf.exp(0.5 * log_sigma2)\n",
    "    return mu + (sigma * eps)\n",
    "\n",
    "def train(iterations):\n",
    "    losses = []\n",
    "    for iteration in range (iterations + 1):\n",
    "        total_loss = 0\n",
    "        for batch in dataset:\n",
    "            with tf.GradientTape() as tape:\n",
    "                z_notrick = encoder(batch)\n",
    "                mu, log_sigma2 = z_notrick[..., :latent_size], z_notrick[..., latent_size:]\n",
    "                z = parameterization_trick(mu, log_sigma2)\n",
    "                reconstructed_x = decoder(z)\n",
    "\n",
    "                reconstruction_loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(batch, reconstructed_x))\n",
    "                kl_loss = -0.5 * tf.reduce_mean(1 + log_sigma2 - tf.square(mu) - tf.exp(log_sigma2))\n",
    "                loss = reconstruction_loss + kl_loss\n",
    "            gradients = tape.gradient(loss, encoder.trainable_variables + decoder.trainable_variables)\n",
    "            enopti.apply_gradients(zip(gradients[:len(encoder.trainable_variables)], encoder.trainable_variables))\n",
    "            deopti.apply_gradients(zip(gradients[len(encoder.trainable_variables):], decoder.trainable_variables))\n",
    "            \n",
    "            total_loss += loss.numpy()\n",
    "        avg_loss = total_loss / batch_size\n",
    "        losses.append(avg_loss)\n",
    "\n",
    "        if (iteration % 1 == 0):\n",
    "            print(f\"Iteration {iteration}: Loss = {avg_loss:.4f}\")\n",
    "\n",
    "    return losses\n",
    "\n",
    "losses = train(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ploss():\n",
    "    plt.plot(losses, label=\"Loss\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss Curve\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "ploss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss(num_samples=1000):\n",
    "    z = tf.random.normal(shape=(num_samples, latent_size)).numpy().flatten()\n",
    "\n",
    "    plt.hist(z, bins=28)\n",
    "    plt.xlabel(\"Value z\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "\n",
    "gauss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_image(rows, cols):\n",
    "    num_images = rows * cols\n",
    "    z = tf.random.normal(shape=(num_images, latent_size))\n",
    "    reconstructed_x = decoder(z).numpy().reshape(num_images, 28, 28)\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 2, rows * 2))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(reconstructed_x[i], cmap=\"gray\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "gauss_image(2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_z(idx):\n",
    "    img = images[idx:idx+1]\n",
    "    z_notrick = encoder(img)\n",
    "    mu, log_sigma2 = z_notrick[..., :latent_size], z_notrick[..., latent_size:]\n",
    "    z = parameterization_trick(mu, log_sigma2).numpy().flatten()\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.hist(z, bins=28)\n",
    "    plt.xlabel(\"Valuez z\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "\n",
    "my_z(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_z_image(idx=0):\n",
    "    img = images[idx:idx+1]\n",
    "    z_notrick = encoder(img)\n",
    "    mu, log_sigma2 = z_notrick[..., :latent_size], z_notrick[..., latent_size:]\n",
    "    z = parameterization_trick(mu, log_sigma2)\n",
    "\n",
    "    reconstructed_x = decoder(z)\n",
    "    reconstructed_img = reconstructed_x.numpy().reshape(rows, cols)\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(reconstructed_img, cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "my_z_image(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
